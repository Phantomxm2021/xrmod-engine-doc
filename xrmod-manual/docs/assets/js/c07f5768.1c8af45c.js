"use strict";(self.webpackChunkxrmod_manual=self.webpackChunkxrmod_manual||[]).push([[8976],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>p});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),d=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),u=d(n),p=r,h=u["".concat(s,".").concat(p)]||u[p]||m[p]||i;return n?a.createElement(h,l(l({ref:t},c),{},{components:n})):a.createElement(h,l({ref:t},c))}));function p(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=u;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var d=2;d<i;d++)l[d]=n[d];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},10375:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>c});var a=n(87462),r=(n(67294),n(3905)),i=n(61422);const l={sidebar_position:5,title:"Features"},o=void 0,s={unversionedId:"arblock/features",id:"arblock/features",title:"Features",description:"Coaching Overlay",source:"@site/docs/experience-manual/arblock/features.mdx",sourceDirName:"arblock",slug:"/arblock/features",permalink:"/experience-manual/arblock/features",draft:!1,editUrl:"https://github.com/Phantomxm2021/xrmod-engine-doc/tree/main/docs/experience-manual/arblock/features.mdx",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,title:"Features"},sidebar:"tutorialSidebar",previous:{title:"Visualizer Block",permalink:"/experience-manual/arblock/visualizer-block"},next:{title:"Graphics",permalink:"/experience-manual/arblock/graphics"}},d={},c=[{value:"Coaching Overlay",id:"coaching-overlay",level:2},{value:"Environment Probe",id:"environment-probe",level:2},{value:"Light Estimation",id:"light-estimation",level:2},{value:"Post Processing",id:"post-processing",level:2},{value:"AR Interaction",id:"ar-interaction",level:2},{value:"Immersal Setting",id:"immersal-setting",level:2},{value:"Use Server Localizer",id:"use-server-localizer",level:3},{value:"Turn Off Server Localized",id:"turn-off-server-localized",level:3},{value:"Localization Interval",id:"localization-interval",level:3},{value:"Render Mode",id:"render-mode",level:3},{value:"Face Mesh",id:"face-mesh",level:2},{value:"Maximum Face Count",id:"maximum-face-count",level:3},{value:"Occlusion",id:"occlusion",level:2},{value:"Enable AR Occlusion",id:"enable-ar-occlusion",level:3},{value:"Environment Depth Mode",id:"environment-depth-mode",level:3},{value:"Human Segmentation Depth Mode",id:"human-segmentation-depth-mode",level:3},{value:"Human Segmentation Stencil Mode",id:"human-segmentation-stencil-mode",level:3},{value:"Occlusion Preference Mode",id:"occlusion-preference-mode",level:3},{value:"ARWorld Scale Block",id:"arworld-scale-block",level:2},{value:"Multiplayer Block",id:"multiplayer-block",level:2}],m={toc:c};function u(e){let{components:t,...l}=e;return(0,r.kt)("wrapper",(0,a.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"coaching-overlay"},"Coaching Overlay"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://developer.apple.com/documentation/arkit/arcoachingoverlayview"},"This view")," provides a standardized installation procedure for your users. You can configure this view to automatically display during session initialization and limited tracking, while giving users specific instructions to best facilitate ARKit's world tracking."),(0,r.kt)("coverimg",{url:n(50009),height:"25rem",padding:"0"}),(0,r.kt)("h2",{id:"environment-probe"},"Environment Probe"),(0,r.kt)("p",null,"Environmental detection is a technology that captures real-world images from a camera and organizes this information into environmental textures, such as a cube map, which contains views in all directions from a specific point in the scene. Using this environment texture to render 3D objects can reflect real-world images in the rendered objects. The result is usually real reflections and lighting of virtual objects, influenced by perceptions of the real world."),(0,r.kt)("coverimg",{url:n(12213),height:"25rem",padding:"0"}),(0,r.kt)("h2",{id:"light-estimation"},"Light Estimation"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://developer.apple.com/documentation/arkit/camera_lighting_and_effects/adding_realistic_reflections_to_an_ar_experience"},"Light Estimation")," is discrete visual cues for a given image and provide detailed information about the lighting in a given scene. You can then use this information when rendering virtual objects to illuminate them under the same conditions as the placed scene, so that these objects feel more realistic and enhance the user's immersive experience."),(0,r.kt)("coverimg",{url:n(4035),height:"25rem",padding:"0"}),(0,r.kt)("h2",{id:"post-processing"},"Post Processing"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@7.1/manual/integration-with-post-processing.html"},"Post-processing")," is the process of applying full-screen filters and effects before the camera's image buffer is displayed on the screen. It can greatly improve the visual effect of your product with very little setup time.\nYou can use post-processing effects to simulate physical camera and film propertie"),(0,r.kt)("coverimg",{url:n(65219),height:"25rem",padding:"0"}),(0,r.kt)("h2",{id:"ar-interaction"},"AR Interaction"),(0,r.kt)("p",null,"AR Interaction is an advanced, component-based interaction system. It also contains auxiliary components, you can use these components to extend the function of drawing visual effects and hooking your own interactive events."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"AR gesture system maps screen touches to gesture events"),(0,r.kt)("li",{parentName:"ul"},"AR gesture interactor and interactive conversion gestures, such as position, selection, conversion, rotation and zoom to object operations"),(0,r.kt)("li",{parentName:"ul"},"AR annotations inform users about AR objects placed in the real world")),(0,r.kt)(i.Z,{src:"/static/videos/arinteraction.mp4",className:"custom-video-showcase",mdxType:"VideoPlayer"}),(0,r.kt)("h2",{id:"immersal-setting"},"Immersal Setting"),(0,r.kt)("p",null,"Cre\xadate AR expe\xadri\xadences that merge dig\xadi\xadtal and phys\xadi\xadcal real\xadi\xadties for con\xadsumers, indus\xadtri\xadal use, ad cam\xadpaigns and more. "),(0,r.kt)("coverimg",{url:n(22916),height:"25rem",padding:"0"}),(0,r.kt)("h3",{id:"use-server-localizer"},"Use Server Localizer"),(0,r.kt)("p",null,"The environment where the current user is located is located through the data stored on the cloud server to load the grid map of the current environment."),(0,r.kt)("h3",{id:"turn-off-server-localized"},"Turn Off Server Localized"),(0,r.kt)("p",null,"When the map is located through the cloud, stop using the cloud location to prevent excessive API calls from increasing costs."),(0,r.kt)("h3",{id:"localization-interval"},"Localization Interval"),(0,r.kt)("p",null,"Cloud positioning/device positioning interval frequency"),(0,r.kt)("h3",{id:"render-mode"},"Render Mode"),(0,r.kt)("p",null,"Mode of rendering point cloud data."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Render Mode"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Do Not Render"),(0,r.kt)("td",{parentName:"tr",align:null},"Do not render scene point cloud data")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Editor Only"),(0,r.kt)("td",{parentName:"tr",align:null},"Only render in Unity Editor (not in Play mode)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Editor And Runtime"),(0,r.kt)("td",{parentName:"tr",align:null},"Always render, whether it is Editor-Play or when it is running on the device")))),(0,r.kt)("h2",{id:"face-mesh"},"Face Mesh"),(0,r.kt)("p",null,"The face mesh is a 3D model of a face. It works in combination with the face tracker in XRMOD Engine to create a surface that reconstructs someone's expressions."),(0,r.kt)("coverimg",{url:n(99948),height:"25rem",padding:"0"}),(0,r.kt)("h3",{id:"maximum-face-count"},"Maximum Face Count"),(0,r.kt)("p",null,"Maximum number of faces that can be tracked"),(0,r.kt)("h2",{id:"occlusion"},"Occlusion"),(0,r.kt)("p",null,"Occlusion: Allows the virtual content to be occluded by the detected environment depth (environmental occlusion) or the detected human body depth (human occlusion). Some devices provide in-depth information about the real world. For example, there is a feature called person occlusion. iOS devices with A12 bionic chip (newer) can detect the depth information of the human body in the frame of the augmented reality camera. Newer Android phones and iOS devices are equipped with lidar scanners that can provide depth images of the environment, and each pixel contains a depth estimate between the device and the physical environment."),(0,r.kt)("coverimg",{url:n(81326),height:"25rem",padding:"0"}),(0,r.kt)("h3",{id:"enable-ar-occlusion"},"Enable AR Occlusion"),(0,r.kt)("p",null,"Enable AR Occlusion, it will turn on the Occlusion algorithm."),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"This feature is quite costly in performance and memory, please use it with caution")),(0,r.kt)("h3",{id:"environment-depth-mode"},"Environment Depth Mode"),(0,r.kt)("p",null,"Environment segmentation depth algorithm mode"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Mode"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Disabled"),(0,r.kt)("td",{parentName:"tr",align:null},"Environment depth is disabled and will not be generated")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Fastest"),(0,r.kt)("td",{parentName:"tr",align:null},"Environment depth is enabled and will be generated at the fastest resolution")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Medium"),(0,r.kt)("td",{parentName:"tr",align:null},"Ambient depth is enabled and will be generated at medium resolution")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Best"),(0,r.kt)("td",{parentName:"tr",align:null},"Ambient depth is enabled and will be generated at the best resolution")))),(0,r.kt)("h3",{id:"human-segmentation-depth-mode"},"Human Segmentation Depth Mode"),(0,r.kt)("p",null,"Human body segmentation depth algorithm mode"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Mode"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Disabled"),(0,r.kt)("td",{parentName:"tr",align:null},"Segmentation depth is disabled and will not be generated")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Fastest"),(0,r.kt)("td",{parentName:"tr",align:null},"Segmentation depth is enabled and will be generated without additional image filtering")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Best"),(0,r.kt)("td",{parentName:"tr",align:null},"Segmentation depth is enabled and will produce additional image filtering")))),(0,r.kt)("h3",{id:"human-segmentation-stencil-mode"},"Human Segmentation Stencil Mode"),(0,r.kt)("p",null,"Human body segmentation template algorithm mode"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Mode"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Disabled"),(0,r.kt)("td",{parentName:"tr",align:null},"The split template is disabled and will not be generated")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Fastest"),(0,r.kt)("td",{parentName:"tr",align:null},"The split template is enabled and will be generated in the fastest resolution")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Medium"),(0,r.kt)("td",{parentName:"tr",align:null},"The split template is enabled and will be generated in moderate resolution")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Best"),(0,r.kt)("td",{parentName:"tr",align:null},"The split template is enabled and will be generated with the best resolution")))),(0,r.kt)("h3",{id:"occlusion-preference-mode"},"Occlusion Preference Mode"),(0,r.kt)("p",null,"AR blocking algorithm preference mode\n|Mode|Description|\n|---|---|\n|Prefer Environment Occlusion|The preferred is the use of the environment depth occlusion|\n|Prefer Human Occlusion|Priority use human split templates and depth to block|"),(0,r.kt)("h2",{id:"arworld-scale-block"},"ARWorld Scale Block"),(0,r.kt)("p",null,"AR World Scale scales the size of the current AR experience scene according to the size value set by the developer to get a better visual effect and experience."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"AR World Scale"),(0,r.kt)("td",{parentName:"tr",align:null},"Float"),(0,r.kt)("td",{parentName:"tr",align:null},"The size value of the AR experience scene world at runtime.  Unity 1 unit is equal to 1 meter in the real world. Default scale size is 1.")))),(0,r.kt)("h2",{id:"multiplayer-block"},"Multiplayer Block"),(0,r.kt)("p",null,"Multiplayer Block is a system for building multiplayer capabilities for AR experiences."),(0,r.kt)("coverimg",{url:n(64333),height:"25rem",padding:"0"}))}u.isMDXComponent=!0},61422:(e,t,n)=>{n.d(t,{Z:()=>l});var a=n(87462),r=n(67294),i=n(86010);function l(e){let{src:t,mobile:n,className:l,...o}=e;return r.createElement("video",(0,a.Z)({className:(0,i.Z)("dyte-video-showcase",l,n&&"mobile"),src:t,autoPlay:!0,loop:!0,controls:!1,muted:!0},o))}},50009:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/coachingoverlayblock-bb407aa91adc9321395ad1da054b8541.png"},12213:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/environment-probe-b747bffdf6e4a981f84dd9853e9be9fc.png"},99948:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/facemesh-22d81cb62558966b7368dd56c5ce729d.jpeg"},22916:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/immersalsdk-f892276d6b38a4211e831459a6a96b4c.jpg"},4035:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/lightestimation-e2d96d551449a562c91549c14b994550.png"},64333:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/multiplayerblock-a3a0e1e7e16cae299e0f1f4b97947420.png"},81326:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/occlusion-4b31534863bfddb09cdaac9335a70b20.jpg"},65219:(e,t,n)=>{n.r(t),n.d(t,{default:()=>a});const a=n.p+"assets/images/postprocessing-e3837c1589bff76d25885637b25f9cae.jpg"}}]);